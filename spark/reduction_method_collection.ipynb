{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "from operator import add\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('test')\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------\n",
    "\n",
    "### additional\n",
    "\n",
    "### 어떤 일의 순위가 있는 경우 병렬처리가 힘들다. \n",
    "\n",
    "### t1 -> t2 와 같이 t2 가 이루어지기 위해 t1 이 필요한 경우 병렬처리가 힘들다.\n",
    "\n",
    "### 즉 각각의 task 들이 독립적으로 작업이 가능할 때 병렬상태이다. 라고 이해하면 좋겠다.\n",
    "\n",
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Actions\n",
    "### 근접하는 elements 들을 모아서 하나의 결과로 만든다.\n",
    "__methods__ 와 파라미터는 다음과 같고, 코드로 하나씩 살펴보자. \n",
    "\n",
    "1. .reduce()\n",
    "\n",
    "   ex - ```RDD.reduce(function)```\n",
    "\n",
    "2. .fold()\n",
    "\n",
    "   ex - ```RDD.fold(startvalue(aka. zerovalue), function)```\n",
    "\n",
    "3. .groupby()\n",
    "\n",
    "   ex - ```RDD.groupBy(function)```\n",
    "\n",
    "4. .aggregate()\n",
    "\n",
    "   ex - ```RDD.aggregate(startvalue, seqOp(change type), combOp(reduction function : add, lambda x,y,z: ~))```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "rdd1.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 알 수 있 듯, reduce 라는 method 는 사용자가 지정한 함수를 받아서 단일 값으로 줄여주는 역할을 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드도 처음의 코드와 같은 역할을 하는 것을 알 수 있다. 즉 어떤 함수의 내용을 적용받아 하나의 단일 값으로 바꿔준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(['he', 'll', 'o', '!'])\n",
    "\n",
    "rdd2.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "string type 도 가능하다는 점을 보여준다.\n",
    "\n",
    "좀 더 여러운 버전도 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = sc.parallelize([1,2,3,4], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드는 RDD 를 2 개의 파티션으로 분할해서 만든다 라는 코드이다.\n",
    "\n",
    "```sc.parallelize([dataframe, list, array], n)``` 이런 식인데 대충 느낌이 오지 않나??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.reduce(lambda x, y: (x*2) + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드의 연산 방식을 살펴보면 파티션을 두개로 나누니까 [1,2], [3,4] 로 나뉘었을 것이고.. \n",
    "\n",
    "첫 번째 파티션의 결과는 1*2 + 2 = 4\n",
    "\n",
    "두 번째 파티션의 결과는 3*2 + 4 = 10\n",
    "\n",
    "그리고 두 개의 파티션의 결과는 4*2 + 10 = 18 이 나온 것이다. 처음에는 이해하기 힘들더라...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드처럼 RDD 의 elements 가 홀수인데 파티셔닝을 짝수로 or RDD 의 elements 가 짝수인데 파티셔닝을 홀수로 하면 어떻게 될까..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.reduce(lambda x, y: (x*2) + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case 1. [1,2,3] , [4,5] 로 분할된 경우 .... 는 없다.\n",
    "\n",
    "1 * 2 + 2 = 4\n",
    "\n",
    "4 * 2 + 3 = 11\n",
    "\n",
    "4 * 2 + 5 = 13\n",
    "\n",
    "22 + 13 = 35 된다면 대략 이런 식으로 계산되지 않을까?\n",
    "\n",
    "__홀짝 or 짝홀의 경우 무조건 마지막 번째의 파티션이 많은 기준으로 파티셔닝 하기 때문__ -> chatgpt 출처\n",
    "\n",
    "case 2. [1,2] , [3,4,5] 파티셔닝한다. \n",
    "\n",
    "1 * 2 + 2 = 4\n",
    "\n",
    "3 * 2 + 4 = 10\n",
    "\n",
    "10 * 2 + 5 = 25\n",
    "\n",
    "4 * 2 + 25 = 33\n",
    "\n",
    "case 2. 와 같은 방식으로 연산하게 된다. \n",
    "\n",
    "case 1. 과 같은 방식으로는 연산되지 않는다는 점을 기억하자. \n",
    "\n",
    "솔직히 chatgpt 를 무한정 신뢰할 수는 없다. 근데 계산결과가 그렇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = sc.parallelize([1,2,3,4,5], 3)\n",
    "\n",
    "rdd5.reduce(lambda x, y: (x*2) + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1, [2, 3] , [4, 5]] 으로 파티셔닝하고,\n",
    "\n",
    "1    첫 번째 파티션 계산 결과    ->   __짜투리는 y 에 해당하는 값이 존재하지 않아 연산하지 않는다.__\n",
    "\n",
    "2 * 2 + 3 = 7 두 번째 파티션 계산 결과\n",
    "\n",
    "4 * 2 + 5 = 13 세 번째 파티션 계산 결과\n",
    "\n",
    "1(1p) * 2 + 7(2p) = 9 \n",
    "\n",
    "9(1p,2p 연산 결과) * 2 + 13(3p 연산 결과) = 31 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "솔직히 아직도 어떻게 파티셔닝이 될 지는 잘 모르겠다. \n",
    "\n",
    "위에 기술한 방식도 chatgpt 에서 정보를 받아 해보기도 하고, 몇차례 혼자 이것 저것 넣어서 실험한 결과라..\n",
    "\n",
    "chatgpt 의 답변도 일관되지 않았고, 구글링을 해도 의견이 분분했다.\n",
    "\n",
    "그래서 개인적으로는 연산의 순서와 상관 없이, 결과값을 보장하기 위해서는 교환법칙, 결합법칙을 생각하며 코딩하는 것이 좋겠다.\n",
    "\n",
    "__교환 법칙 = (a * b) = (b * a) 순서를 바꿔서 곱해도 값에 변화가 없다.__\n",
    "\n",
    "__결합 법칙 = (a * b) * c = a * (b * c) 어떻게 묶어서 곱하더라도 값에 변화가 없다.__\n",
    "\n",
    "솔직히 위의 법칙들은 초등학교 때 배우지 않나?? 저걸 여기서 생각해야 할 줄은 몰랐다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이건 좀 쉽다. start value 라고 적어두었던 파라미터는 RDD 를 n 개로 파티셔닝 했을 때 그 각각의 파티셔닝 안의 0 번째 인덱스들 이라고 이해하면 되겠다.\n",
    "\n",
    "그래서 아까 rdd1 은 1 개의 파티션 [1,2,3,4,5] 였었고 add function 을 이용해 각 요소들을 더해 1개의 값으로 축소한 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd5 = sc.parallelize([1,2,3,4,5], 3)\n",
    "\n",
    "rdd5.fold(0, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 연산도 보면 \n",
    "\n",
    "파티셔닝이 [1, [2, 3], [4, 5]] 일 때,\n",
    "\n",
    "[0 [0, 1] [0, 2, 3], [0, 4, 5]] 이렇게 되어 연산된다고 보면 되겠다. \n",
    "\n",
    "start-value 인 0, 과 파티션 3 개에 0이 들어간 모습이다. 순서는 솔직히 잘 모르겠다.\n",
    "\n",
    "결국 reduce 와의 차이는 첫 번째 값으로 zero-value 가 들어간다는 점만 차이가 있다.\n",
    "\n",
    "zero-value 라고 꼭 0 만 넣는 것은 아니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.fold(2, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드의 계산 결과를 보면 유추가 가능하다.\n",
    "\n",
    "파티션 3 개에 2 라는 start-value 가 추가돼 15 에서 6 만큼 늘어난 21 이 아닌\n",
    "\n",
    "zero-value = 2\n",
    "\n",
    "(각 파티션 3 개에 추가된 2) = 3 * 2\n",
    "\n",
    "그래서 15 + 8 = 23 이라는 결과값이 나온다.\n",
    "\n",
    "상식적으로는 21 이 맞는 것 같은데 결과값이 23 이 나와 유추해본 결과이다.\n",
    "\n",
    "23 이 되려면 파티셔닝이 4 개로 되는 것 아닌가..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.fold(2, lambda x, y: x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 [2, 1] [2, 2, 3], [2, 4, 5]] 라고 가정하고 계산해보면\n",
    "\n",
    "2 * 2 * 12 * 40 = 1920  이렇게 계산하나봐요.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x22d638368b0>),\n",
       " (0, <pyspark.resultiterable.ResultIterable at 0x22d63836ee0>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6 = sc.parallelize([1, 2, 2, 3, 3, 4, 5, 6, 7, 7])\n",
    "\n",
    "res = rdd6.groupBy(lambda x: (x%2)).collect()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 결과로,\n",
    "\n",
    "나머지가 0 인 애들인 iterable 한 pyspark의 내부 클래스와\n",
    "\n",
    "나머지가 1 인 애들인 iterable 한 pyspark의 내부 클래스가 나온다. \n",
    "\n",
    "iterable 한 pyspark의 내부 클래스들을 python object 로 변경해서 살펴보면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 3, 3, 5, 7, 7], [2, 2, 4, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(res[0][1]), list(res[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 2, 4, 6]), (1, [1, 3, 3, 5, 7, 7])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, sorted(y)) for (x, y) in res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 보면 \n",
    "\n",
    "x 에 해당하는 값은 각 튜플의 0 번 인덱스의 값이고, < 후에 설명할 key 에 해당하는 값>\n",
    "\n",
    "y 에 해당하는 값들은 iterable 한 pyspark 집합들 < 후에 설명할 value 에 해당하는 값> 인데 python object 로 바꿔서 확인한 결과이다.\n",
    "\n",
    "문법이 생소할 수도 있어 설명하면, 리스트 컴프리헨션이라는 것이고 파이썬의 장점이기도 한 코드이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. aggregate \n",
    "\n",
    "개인적으로 좀 이해가 힘들더라. 좀 복잡하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7 = sc.parallelize([1, 2, 3, 4])\n",
    "\n",
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "rdd7.aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate 의 파라미터를 remind 하면 \n",
    "\n",
    "rdd.aggregate(zero-value, seqOp, combOp) 였다. \n",
    "\n",
    "zero-value 는 알고 있 듯 start - value 이고,\n",
    "\n",
    "seqOp 는 타입을 변경하는 함수이며,\n",
    "\n",
    "__seqOp = (lambda x, y: (x[0] + y, x[1] + 1))__\n",
    "\n",
    "combOp 는 합을 하는 함수였다.\n",
    "\n",
    "__combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))__\n",
    "\n",
    "연산 결과를 추적해보면 \n",
    "\n",
    "start - value (0,0) 으로 pair 하게 파티셔닝한다.\n",
    "\n",
    "그래서 RDD 를 [1, 2] 와 [3, 4] 로 pair 하게 파티셔닝한다.\n",
    "\n",
    "__zero - value 는 x[0] = 0, x[1] = 0 이다__\n",
    "\n",
    "seq 연산한 후, combOp 로 합쳐보자\n",
    "\n",
    "[1, 2] 의 seq 의 결과는 다음과 같다.\n",
    "\n",
    "x[0] + y, x[1] + 1 = 0 + 1, 0 + 1 이고 (1, 1) \n",
    "\n",
    "x[0] + y, x[1] + 1 = 1 + 2, 1 + 1 -> (3, 2) 이 된다.\n",
    "\n",
    "첫 번째 파티션의 seq 결과 (3, 2)\n",
    "\n",
    "[3, 4] 의 seq 의 결과는 다음과 같다.\n",
    "\n",
    "x[0] + y, x[1] + 1 = 0 + 3, 0 + 1  이고 (3, 1) \n",
    "\n",
    "x[0] + y, x[1] + 1 = 3 + 4, 1 + 1 -> (7, 2) 이 된다.\n",
    "\n",
    "두 번째 파티션의 seq 결과 (7, 2)\n",
    "\n",
    "마지막 combOp 로 그 둘의 결과를 합치면 \n",
    "\n",
    "(3, 2) + (7, 2) = (10, 4) 가 된다. \n",
    "\n",
    "코드가 뭔가 복잡하고 연산 방식이 처음엔 눈에 잘 들어오지 않고 튕겨져 나가더라.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리.\n",
    "\n",
    "위의 4 개의 코드들은 필수적으로 알아야 하는 reduction 코드들이며 익숙해져서 자유롭게 사용하게 할 수 있게끔 해야 한다고 생각한다. \n",
    "\n",
    "크고 복잡한 데이터들을 정제된 데이터로 축약하는 한마디로 EDA 를 하는 코드들이라 볼 수 있겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newkaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
