{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('transform_action')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.startTime', '1690546919066'),\n",
       " ('spark.app.id', 'local-1690546921789'),\n",
       " ('spark.driver.host', '192.168.124.100'),\n",
       " ('spark.driver.port', '63927'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'transform_action'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 지금 쓰는 세팅\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.124.100:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>transform_action</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=transform_action>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make RDD .parallelize : list -> RDD, .textFile : text -> RDD\n",
    "\n",
    "sample = sc.parallelize(['a','b','c','d','e','f','a1','b2','c3','d4','e5','f6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'a1', 'b2', 'c3', 'd4', 'e5', 'f6']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action part - eager execution\n",
    "\n",
    "sample.collect() \n",
    "\n",
    "# 개발, 디버깅 단계에서 사용. \n",
    "\n",
    "# PRODUCTION 단계에서는 RDD의 리스트 전부를 가져오기 때문에 낭비가 심해 SPARK를 쓰는 의미가 없어짐 지양\n",
    "\n",
    "# 기본적으로 spark 는 규모가 매우 큰 데이터를 효율적으로 다루기 위해 사용하는 것인데,\n",
    "\n",
    "# .collect()의 결과를 보면 N 의 시간복잡도를 가지는 메소드를 production 단계에서 사용하는 것은 지양해야한다고 생각. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'a': 1,\n",
       "             'b': 1,\n",
       "             'c': 1,\n",
       "             'd': 1,\n",
       "             'e': 1,\n",
       "             'f': 1,\n",
       "             'a1': 1,\n",
       "             'b2': 1,\n",
       "             'c3': 1,\n",
       "             'd4': 1,\n",
       "             'e5': 1,\n",
       "             'f6': 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.countByValue()\n",
    "\n",
    "# key 와 value 상태로 개수를 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.take(5)\n",
    "\n",
    "# index 순으로 element를 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.first() \n",
    "\n",
    "# RDD 내 첫 번째 element 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count()\n",
    "\n",
    "# element의 총 개수를 알려줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[11] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.distinct()\n",
    "\n",
    "# distinct 는 transformation 이기에 바로 확인할 수 없고 action을 섞어 써야보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'a1', 'b2', 'c3', 'd4', 'e5', 'f6']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.distinct().collect()\n",
    "\n",
    "# sql의 distinct 와 같은 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.foreach(lambda x: print(x))\n",
    "\n",
    "# foreach 는 worker node 에서 실행됨\n",
    "\n",
    "# spark context가 있는 driver 에서는 볼 수 없음\n",
    "\n",
    "# log 저장 등에 유리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations part - lazy execution\n",
    "\n",
    "# Transformations = narrow + wide\n",
    "\n",
    "# 1. narrow transformation\n",
    "\n",
    "# 1:1 변환, .filter(), .map(), .flatmap(), .sample(), .union()\n",
    "\n",
    "# 열을 조작하기 위해 다른 열 / 파티션의 데이터를 사용하지 않음\n",
    "\n",
    "# 정렬이 필요하지 않은 경우 <정렬은 다른 열의 데이터를 사용>\n",
    "\n",
    "# 2. wide transformation\n",
    "\n",
    "# resource를 비교적 많이 필요로 함.\n",
    "\n",
    "# shuffling, intersection, join, distinct, cartesian, .reduceBykey(), groupBykey()\n",
    "\n",
    "# 아웃풋 RDD의 파티션에 조건,동작시킨 행위에 따라 다른 데이터가 들어갈 수 있게됨.\n",
    "\n",
    "# 위에 action part 에서 말했던 것처럼, action 에 해당하는 메소드들은 client로 가져오기 때문에\n",
    "\n",
    "# python object 로 처리할 수 있지만, transformation 에 해당하는 메소드들은 RDD를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow t 1. .map()\n",
    "\n",
    "sample.map(lambda x: x+'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ae', 'be', 'ce']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.map(lambda x: x+'e').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[24] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow t 2. .flatmap()\n",
    "\n",
    "corpus = [\"yes yes\", 'No No', \"Yes No\", 'No yes']\n",
    "\n",
    "corpusRDD = sc.parallelize(corpus)\n",
    "\n",
    "corpusRDD.flatMap(lambda x:x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', 'yes', 'No', 'No', 'Yes', 'No', 'No', 'yes']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusRDD.flatMap(lambda x:x.split(\" \")).collect()\n",
    "\n",
    "# 리스트 내부의 값을 어떤 기준에 따라 element를 늘릴 수 있도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10, 12, 14, 16]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2, 3], [4, 5], [6, 7, 8]]\n",
    "dataRDD = sc.parallelize(data)\n",
    "\n",
    "result = dataRDD.flatMap(lambda x: [i * 2 for i in x])\n",
    "result.collect()\n",
    "\n",
    "# 아래의 결과와 같이 몇 겹의 리스트여도, 한 개의 리스트에 한 개의 element 화 하는 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[33] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow t 2. .filter()\n",
    "\n",
    "\n",
    "doubled_corpusRDD = corpusRDD.flatMap(lambda x:x.split(\" \"))\n",
    "\n",
    "doubled_corpusRDD.filter(lambda x:x != 'yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No', 'No', 'Yes', 'No', 'No']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubled_corpusRDD.filter(lambda x:x != 'yes').collect()\n",
    "\n",
    "# 설명 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ParallelCollectionRDD[53] at readRDDFromFile at PythonRDD.scala:274,\n",
       " ParallelCollectionRDD[54] at readRDDFromFile at PythonRDD.scala:274)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wide t 1. .intersection() \n",
    "\n",
    "# 입력 RDD들 간에 데이터의 재분배를 수행하기 때문.\n",
    "\n",
    "# resource 가 많이 필요함.\n",
    "\n",
    "a = [1,2,3,4,5,6]\n",
    "b = [5,6,7,8,9,10]\n",
    "\n",
    "RDD1 = sc.parallelize(a)\n",
    "RDD2 = sc.parallelize(b)\n",
    "\n",
    "RDD1, RDD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD1.intersection(RDD2).collect() # 교집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wide t 2. .union() \n",
    "\n",
    "# 입력 RDD들 간에 데이터의 재분배를 수행하기 때문.\n",
    "\n",
    "RDD1.union(RDD2).collect() # 합집합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 1, 3]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wide t 3. .substract() \n",
    "\n",
    "RDD1.subtract(RDD2).collect() #RDD1 과 RDD2의 intersection 값을 제외한 나머지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 6, 10]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow t 3. .sample() \n",
    "\n",
    "# sample(1. True or False<복원, 비복원 추출>, 2. %<얼마나>, 3. seed 값 설정)\n",
    "\n",
    "RDD12 = RDD1.union(RDD2)\n",
    "\n",
    "RDD12.sample(True, .5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[108] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wide t 3. .groupBy()\n",
    "\n",
    "# 데이터를 새롭게 분배시켜 만든 RDD \n",
    "\n",
    "corpusRDD.groupBy(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('y', <pyspark.resultiterable.ResultIterable at 0x16e40b5a910>),\n",
       " ('N', <pyspark.resultiterable.ResultIterable at 0x16e40b5a9d0>),\n",
       " ('Y', <pyspark.resultiterable.ResultIterable at 0x16e40b5aa60>)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = corpusRDD.groupBy(lambda x:x[0]).collect()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key is :y, value is : <pyspark.resultiterable.ResultIterable object at 0x0000016E40B5A910>\n",
      "key is :N, value is : <pyspark.resultiterable.ResultIterable object at 0x0000016E40B5A9D0>\n",
      "key is :Y, value is : <pyspark.resultiterable.ResultIterable object at 0x0000016E40B5AA60>\n"
     ]
    }
   ],
   "source": [
    "for (k, v) in test:\n",
    "    print(f\"key is :{k}, value is : {v}\")\n",
    "\n",
    "# 그냥 v 를 쓰면 iterable한 pyspark result 기 때문에 python object로 바꿔주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key is :y, value is : ['yes yes']\n",
      "key is :N, value is : ['No No', 'No yes']\n",
      "key is :Y, value is : ['Yes No']\n"
     ]
    }
   ],
   "source": [
    "for (k, v) in test:\n",
    "    print(f\"key is :{k}, value is : {list(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x16e40b5b9d0>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x16e40b6e160>)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RDD12.groupBy(lambda x:x%2).collect()\n",
    "\n",
    "# 나누어 떨어진 것과 1이 남은 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 6, 8, 10]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(RDD12.groupBy(lambda x:x%2).collect()[0][1])\n",
    "\n",
    "# 위에서 말했던 iterable 한 RDD result 를 python object 로\n",
    "\n",
    "# 나누어 떨어진 애들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 5, 7, 9]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(RDD12.groupBy(lambda x:x%2).collect()[1][1])\n",
    "\n",
    "# 안되는 애들"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newkaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
